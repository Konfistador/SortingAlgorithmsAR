\documentclass[]{report}
\renewcommand\thesection{\arabic{section}}%for page numbering in arabics
\usepackage{graphicx,tabularx}%for figures and tables
\usepackage[utf8]{inputenc} %allows special characters such as ä, ö, ỳ
\usepackage[english]{babel}  %set the language to English
\usepackage[margin=1.5in]{geometry} %change page margins 
\usepackage{sectsty}%section headers
\usepackage{algpseudocode}
\usepackage{float}
\allsectionsfont{\sffamily\large}
\subsectionfont{\sffamily\normalsize}
\linespread{1.2}% line distance
\usepackage{lipsum}% http://ctan.org/pkg/lipsum
\usepackage{caption}%use for captions on tables
%use this exact command. The style and bibliographystyle has to be authoryear (Havard). The sorting is nyt: name, year, title so that the bibliography is sorted alphabetically. firstinits=true shortens the names: Albert Einstein -> A. Einstein
\usepackage[backend=bibtex,style=authoryear,bibstyle=authoryear,sorting=nyt,firstinits=true]{biblatex}
\setlength\parindent{0pt}%include this so that your paragraphs don't indent automatically
\addbibresource{report.bib} %this attaches your bib-file, your bibliography (must be in the same folder)
\usepackage[compact]{titlesec}%include title formatting package
\setcounter{secnumdepth}{5}

% Title Page
\title{Relationship between Time Complexity and Number of Elements, within different Sorting Algorithms.}
\author{Mino Karadzhov and Kestutis Dikinis}
\date{December 10th 2021 \\Module: SEAR \\Venlo, Limburg, Netherlands}


\begin{document}

\maketitle

\begin{abstract}
This is the abstract. We are going to write this one at the end.

\pagenumbering{roman}

\end{abstract}

\tableofcontents
\setcounter{page}{3}
\listoffigures %UNCOMMENT IF YOU HAVE FIGURES
%\listoftables %UNCOMMENT IF YOU HAVE TABLES
\pagebreak

\pagenumbering{arabic}	
	
\section{Introduction}
TODO: Write about the purpose of this section, when you finish writing it.
  
\subsection{Context and Background}
In computer science, a sorting algorithm is an algorithm that takes elements of a list and puts them into an order - ascending or descending. The most popular values used are numerical and lexicographical order. It is important to have efficient sorting for optimising other algorithms, such as merge and search algorithms, as they require data to be in sorted list.
		- History of Algorithms.
		
		- Modern Usage, developments.
		
		- Maybe some discussion on sorting algorithms(if present somewhere and can be quoted).
\subsection{Sorting algorithm explanations}
		Bubblesort is a simple sorting algorithm that belongs to the family of comparison sorting. It works by repeatedly stepping through the list to be sorted, comparing two items at a time and swapping them if they are in the wrong order. Bubblesort has a worst-case complexity O(n 2 ) and in the best case O(n). Its memory complexity is O(1). 

Heapsort is a comparison-based sorting algorithm, and is part of the Selection sort family. Although somewhat slower in practice on most machines than a good implementation of Quicksort, it has the advantage of a worst-case O(n log n) runtime.

Insertionsort is a naive sorting algorithm that belongs to the family of comparison sorting. In general Insertionsort has a time complexity of O(n 2 ) but is known to be efficient on data sets which are already substantially sorted. Its average complexity is n 2/4 and linear in the best case. Furthermore Insertionsort is an in-place algorithm that requires a constant amount O(1) of memory space. 

Mergesort belongs to the family of comparison-based sorting. It has an average and worst-case performance of O(n log n). Unfortunately, Mergesort requires three times the memory of in-place algorithms such as Insertionsort. 

Quicksort [11] belongs to the family of exchange sorting. On average, Quicksort makes O(n log n) comparisons to sort n items, but in its worst case it requires O(n 2 ) comparisons. Typically, Quicksort is regarded as one of the most efficient algorithms and is therefore typically used for all sorting tasks. Quicksort’s memory usage depends on factors such as choosing the right Pivot-Element, etc. On average, having a recursion depth of O(log n), the memory complexity of Quicksort is O(log n) as well. 

Selection sort belongs to the family of in-place comparison sorting. It typically searches for the minimum value, exchanges it with the value in the first position and repeats the first two steps for the remaining list. On average Selection sort has a O(n 2 ) complexity that makes it inefficient on large lists. Selectionsort typically outperforms Bubblesort but is generally outperformed by Insertionsort.
\subsection{Hypothesis, Problems and Considerations}
		- Our Hypothesis that we are going to see a linear growth of the Time Complexity, towards Size of Collection.
		- What is Time Complexity ?
		- Why is Time Complexity important? 
		-  What could affect the Time Complexity.
		- External factors that can affect the Time Complexity.
		\subsubsection{Time Complexity and the importance of it.}
		\subsubsection{Factors, operating on Time Complexity.}
		\subsubsection{Different methods of approaching our Research question.}
		
\section{Methodology}
In order to approve or deny our initial hypothesis, an Emperical Analysis of certain  Sorting Algorithms was conducted. This chapter of the paper describes the process of conducting the experiment and creating the proper experimental enviornment. The subject group of Sorting Algorithms includes: QuickSort, BubbleSort, SelectionSort and HeapSort. The main goal of the analysis is to observe changes of the TIme Complexity, when the same algorithm is given the task to sort a collection of a certain size. Each observation of our experiment includes a task of sorting, performed by one of the sorting algorithms from our subject group, performed on a collection of certain size.\\
In order to conduct the Emperical Analysis, a custom-made Java application was developed and used in the process. The main purpouse of which is to create a proper experimental enviornment, that is going to allow close observation of each algorithm from our sunject group. \\
The overall goal is to gain an output, which allows data analysis on the covariation between the Size of Collection, used for the observation, and the time that was required for the algorithm to fully perfrom the sorting task.
	\subsection{Working Implemntation of our Algorithms.}
	One requirement of the Empirical Analysis is a working implementation of the Sorting Algorithms,that are going to be used for the experiment. Due to this, the Java programming language was used to create working implementations of each one of the four members of our Subject group. All of the concrete implementations of the Algorithms implement a common interface, towards which the testing is being conducted.
	\subsection{Developing and using our experimental environment.}
	In order to serve the need of a proper experimental enviornment for conducting the Analysis, Petko was developed. Petko is the name of the custom-made  Algorithm Analysis tool, that generates unsorted collections of a fixed size and uses sorters(Sorting Algorithms) to conduct the sorting operation. The latter is being closely monitored and the time required for each sorting is being recorded in an output file, in a format of nanoseconds. \\
	Each Algorithm is used to perform 18 different observations. Each Observation requires the algorithm to sort an unsorted collection of a fixed size. The size of the unordered collection grows with each following observation. The first observation of each algorithm starts with performing a sorting on a Collection with a number of elements of 2, whereas the last(18th) observation provides the Algorithm with an unsorted collection of 262144 elements. The growth of Collection Size is Logarithmic with base 2. Respectively, this means that the first collection for sorting is with size 2$^{1}$, whereas the collection for the last observation has a size equal to 2$^{18}$. \\
	Petko actively uses the Java.time integrated libray, in order to measure the Duration of time between the start and end of the sorting process. That is made possible, by placing an Instant at the Start and End point of each Algorithm execution. The following PseudoCode provides a brief description of the technique:
	\begin{algorithmic}[H]
		\State $startPoint \gets Instant.now()$
		\State $sort()$
		\State $endPoint \gets Instant.now()$
		\State $timeComplexity \gets Duration.between(startPoint, endPoint)$
		\end{algorithmic} 
		 Each Observation records the Sorting Algorithm that is being used, the TimeComplexity of the observation and the Size of Collection. This output is being gathered and recored into an external File under the .csv extension. 
		\subsection{Data Analysis and Visualisation}
		Petko's output file provides a great base for conducting Data Analysis on the results of the observations. By doing this, it is possible to identify and study certain patterns in the covariation between Time Complexity and Size of Collection. The same output file was imported into R and the findings were visualised, thanks to the tidyverse library.
		\section{Results}
	This section provides a detailed overview of the exprimental results.
\subsection{Experimental Results}
After conducting the Emperical Analysis, 72 datapoints were generated. The same visualised on a ScatterPlot have the following view:

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{ResultsOverallView}
	\caption[Figure 3.1:]{Overall view of the Experimental results.}
	\label{fig:resultsoverallview}
\end{figure}

As we can clearly see from the above diagram, the relationship between Time Complexity and Collection Size differes between each one of the Sorters. In addition to this, we can easily identify a sharp growth of BubbleSort and SelectionSort time complexities, when the Collection size becomes more than 20 000 elements. On the other hand, looking at the QuickSort and HeapSort, we can easily observe a relationship, which can be described as similliar to Constant Time Complexity.
Taking the above into account, we can easily identify 2 sub-groups of sorters - One, where  TimeComplexity appears to be constant and One, where we can clearly see a Time Complexity growth, similliar to a Linear Growth.\\
Since the two sub-groups are vastly contrasting on the Overall view, this may be a reason why we are missing certain patterns on our visualisation.

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{OverviewOfConstantTimeComplexityAlgorithms}
	\caption[Figure 3.2]{Focused view on the Constant Time Complexity sub-group.}
	\label{fig:overviewofconstanttimecomplexityalgorithms}
\end{figure}

\newpage	
\section{Discussion}
- Small intro on the purpouse of the section here.
\subsection{Interpretation}
- Extended analysis on our graphs
- Deny or Approve our Hypothesis(Partially aprove it)
\subsection{Considerations}
- Limitations of the Machine used
- Limitations of the Emperical type of Analysis
\subsection{Evaluations}
- Simple Evaluation of our findings
\end{document}
	
	
\printbibliography[title=References]
	
\end{document}          
